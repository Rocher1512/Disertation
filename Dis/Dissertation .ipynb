{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nibabel.testing import data_path\n",
    "img = nib.load(\"ADNI_002_S_0295_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070319113623975_S13408_I45108.nii\")\n",
    "header = img.header\n",
    "#print(header)\n",
    "print(header.get_data_shape())\n",
    "print(header.get_data_dtype())\n",
    "print(header.get_zooms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "img = nib.load(\"ADNI_002_S_0295_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070319113623975_S13408_I45108.nii\")\n",
    "img_data = img.get_fdata()\n",
    "#print(img_data.shape)\n",
    "\n",
    "def show_slices(slices):\n",
    "    fig, axes = plt.subplots(1, len(slices))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n",
    "        \n",
    "slice_0 = img_data[100, :, :]\n",
    "slice_1 = img_data[:, 100, :]\n",
    "slice_2 = img_data[:, :, 50]\n",
    "print(slice_0[32])\n",
    "\n",
    "\n",
    "show_slices([slice_0, slice_1, slice_2])\n",
    "#plt.suptitle(\"Center slices for EPI image\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import scikitplot as skplt\n",
    "\n",
    "listofColumns = []\n",
    "listofIDs = []\n",
    "listofBrainData = []\n",
    "listofBrainTypes = []\n",
    "\n",
    "K_Means_Divisions = 5\n",
    "\n",
    "Data_Brains = pd.read_csv(\"TestDatasheet.csv\")\n",
    "listofColumns = Data_Brains.columns\n",
    "for i in range(len(Data_Brains)):\n",
    "    Subject = Data_Brains.loc[i][\"Subject\"]\n",
    "    Group = Data_Brains.loc[i][\"Group\"]\n",
    "    Acq_Date = Data_Brains.loc[i][\"Acq Date\"]\n",
    "    List_Acq_Date = list(Acq_Date)\n",
    "    for j in range(len(List_Acq_Date)):\n",
    "        if(List_Acq_Date[j] == '/'):\n",
    "            List_Acq_Date[j] = '_'\n",
    "    Acq_Date = \"\".join(List_Acq_Date)\n",
    "    File_Path = (Subject + \"-\" + Group + \"-\" + Acq_Date + \".nii\")\n",
    "    print(File_Path)\n",
    "    File_root= \".\\Test_dataset\"\n",
    "    file = os.path.join(File_root,File_Path)\n",
    "    img = nib.load(file)\n",
    "    img_data = img.get_fdata()\n",
    "    listofIDs.append(i)\n",
    "    listofBrainData.append(img_data)\n",
    "    if(Group == \"CN\"):\n",
    "        listofBrainTypes.append(0)\n",
    "    elif(Group == \"EMCI\"):\n",
    "        listofBrainTypes.append(1)\n",
    "    elif(Group == \"LMCI\"):\n",
    "        listofBrainTypes.append(2)\n",
    "    else:\n",
    "        print(\"Error Subject Identified as: \" + Group)\n",
    "X_T = []# dividing up the input data\n",
    "Y_T = []# dividing up the answers\n",
    "Y_P = []# the predicted value of each model\n",
    "Predict = []# the predicted values of all the models\n",
    "for i in range(K_Means_Divisions):\n",
    "    X_T.append([])\n",
    "    Y_T.append([])\n",
    "    \n",
    "#Ah Yes I do love a bit of code\n",
    "for i in range(K_Means_Divisions):\n",
    "    lower = int((len(listofBrainData)*i)/K_Means_Divisions)\n",
    "    upper = int((len(listofBrainData)*(i+1))/K_Means_Divisions)\n",
    "    for j in range(lower, upper):\n",
    "        X_T[i].append(listofBrainData[j])\n",
    "for i in range(K_Means_Divisions):\n",
    "    train_X = X_T.copy()\n",
    "    train_Y = Y_T.copy()\n",
    "    train_X.pop(i)\n",
    "    train_Y.pop(i)\n",
    "    Y_P = [0,1]#model(train_X,train_Y,X_T[i])\n",
    "    Predict += Y_P\n",
    "        \n",
    "listofBrainTypes\n",
    "Predict\n",
    "#confusion matrix\n",
    "skplt.metrics.confusion_matrix(Predict, listofBrainTypes)\n",
    "skplt.metrics.plot_confusion_matrix(\n",
    "    Predict,\n",
    "    listofBrainTypes,\n",
    "    normalize = False\n",
    ")\n",
    "skplt.metrics.plot_confusion_matrix(\n",
    "    Predict,\n",
    "    listofBrainTypes,\n",
    "    normalize = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import scikitplot as skplt\n",
    "import os\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "listofColumns = []\n",
    "listofIDs = []\n",
    "listofBrainData = []\n",
    "listofBrainTypes = []\n",
    "listofLMCI = []\n",
    "listofEMCI = []\n",
    "listofCN = []\n",
    "\n",
    "K_Means_Divisions = 5\n",
    "CN = 0\n",
    "EMCI = 1\n",
    "LMCI = 2\n",
    "\n",
    "listofLMCI = os.listdir(\".\\DataSet\\LMCI\")\n",
    "listofEMCI = os.listdir(\".\\DataSet\\EMCI\")\n",
    "listofCN = os.listdir(\".\\DataSet\\CN\")\n",
    "\n",
    "\n",
    "def DLmodel(train_X,train_Y,test_X):\n",
    "    model = MLPClassifier(solver='sgd', alpha = 0.0001, hidden_layer_sizes=(70,70,70), activation = 'tanh', learning_rate = 'invscaling')\n",
    "    trained_model = model.fit(train_X, train_Y)\n",
    "    predict = trained_model.predict(test_X)\n",
    "    print(predict)\n",
    "\n",
    "\n",
    "for i in range(len(listofCN)):\n",
    "    #CN\n",
    "    file = os.path.join(\".\\DataSet\\CN\",listofCN[i])\n",
    "    img = nib.load(file)\n",
    "    img_data = img.get_fdata()\n",
    "    listofBrainData.append(img_data)\n",
    "    listofBrainTypes.append(CN)\n",
    "    #EMCI\n",
    "    file = os.path.join(\".\\DataSet\\EMCI\",listofEMCI[i])\n",
    "    img = nib.load(file)\n",
    "    img_data = img.get_fdata()\n",
    "    listofBrainData.append(img_data)\n",
    "    listofBrainTypes.append(EMCI)\n",
    "    #LMCI\n",
    "    file = os.path.join(\".\\DataSet\\LMCI\",listofLMCI[i])\n",
    "    img = nib.load(file)\n",
    "    img_data = img.get_fdata()\n",
    "    listofBrainData.append(img_data)\n",
    "    listofBrainTypes.append(LMCI)\n",
    "    \n",
    "\n",
    "X_T = []# dividing up the input data\n",
    "Y_T = []# dividing up the answers\n",
    "Y_P = []# the predicted value of each model\n",
    "Predict = []# the predicted values of all the models\n",
    "for i in range(K_Means_Divisions):\n",
    "    X_T.append([])\n",
    "    Y_T.append([])\n",
    "    \n",
    "for i in range(K_Means_Divisions):\n",
    "    lower = int((len(listofBrainData)*i)/K_Means_Divisions)\n",
    "    upper = int((len(listofBrainData)*(i+1))/K_Means_Divisions)\n",
    "    for j in range(lower, upper):\n",
    "        X_T[i].append(listofBrainData[j])\n",
    "for i in range(K_Means_Divisions):\n",
    "    train_X = X_T.copy()\n",
    "    train_Y = Y_T.copy()\n",
    "    train_X.pop(i)\n",
    "    train_Y.pop(i)\n",
    "    #DLmodel(train_X,train_Y,X_T[i])\n",
    "    Y_P = [0,1,2,0,1,2]#model(train_X,train_Y,X_T[i])\n",
    "    \n",
    "    Predict += Y_P\n",
    "\n",
    "#confusion matrix\n",
    "#skplt.metrics.confusion_matrix(Predict, listofBrainTypes)\n",
    "#skplt.metrics.plot_confusion_matrix(\n",
    "#    Predict,\n",
    "#    listofBrainTypes,\n",
    "#    normalize = False\n",
    "#)\n",
    "#skplt.metrics.plot_confusion_matrix(\n",
    "#    Predict,\n",
    "#    listofBrainTypes,\n",
    "#    normalize = True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "Start_Slice = 30\n",
    "Mid_Slice = 110\n",
    "End_Slice = 190\n",
    "Step_Slice = 5\n",
    "\n",
    "Brain_Data = []\n",
    "Brains = []\n",
    "Brain_Types = [0,1,2,0,1,2]\n",
    " \n",
    "\n",
    "img = nib.load(\"ADNI_002_S_0295_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070319113623975_S13408_I45108.nii\")\n",
    "img_data = img.get_fdata()\n",
    "for i in range(Start_Slice,Mid_Slice,Step_Slice):\n",
    "    for j in range(len(img_data[i, :, :])):\n",
    "        for k in range(len(img_data[i, :, :][j])):\n",
    "            Brain_Data.append(img_data[i, :, :][j][k])\n",
    "\n",
    "\n",
    "Brains.append(Brain_Data)\n",
    "Brains.append(Brain_Data)\n",
    "Brains.append(Brain_Data)\n",
    "Brains.append(Brain_Data)\n",
    "Brains.append(Brain_Data)\n",
    "Brains.append(Brain_Data)\n",
    "print(\"Here\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(Brains, Brain_Types, test_size=0.16)\n",
    "model = MLPClassifier(solver='sgd', alpha = 0.0001, hidden_layer_sizes=(70,70,70), activation = 'tanh', learning_rate = 'invscaling')\n",
    "trained_model = model.fit(X_train, y_train)\n",
    "predict = trained_model.predict(X_test)\n",
    "print(predict)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nibabel as nb\n",
    "from deepbrain import Extractor\n",
    "import scikitplot as skplt\n",
    "Brain_Size = 1400000\n",
    "CN = 0\n",
    "EMCI = 1\n",
    "LMCI = 2\n",
    "\n",
    "Brain_Data = []\n",
    "Brain_Data_reduced = []\n",
    "Brains = []\n",
    "Brain_Types = []\n",
    "listofLMCI = []\n",
    "listofEMCI = []\n",
    "listofCN = []\n",
    " \n",
    "def Reduce_Brain(img):\n",
    "    print(\"Reducing a brain\")\n",
    "    Brain_Data_Small = []\n",
    "    prob = ext.run(img) \n",
    "    mask = prob > 0.5\n",
    "    for i in range (len(mask)):\n",
    "        for j in range(len(mask[i])):\n",
    "            for k in range(len(mask[i][j])):\n",
    "                if mask[i][j][k]:\n",
    "                    Brain_Data_Small.append(img[i][j][k])\n",
    "    print(type(Brain_Data_Small[0]))\n",
    "    while(len(Brain_Data_Small) < Brain_Size):\n",
    "        if(len(Brain_Data_Small) % 2 == 0):\n",
    "            Brain_Data_Small.append(0)\n",
    "        else:\n",
    "            Brain_Data_Small.insert(0,0)\n",
    "    print(\"the brain is: \",len(Brain_Data_Small))\n",
    "    return Brain_Data_Small\n",
    "\n",
    "listofLMCI = os.listdir(\".\\DataSet\\LMCI\")\n",
    "listofEMCI = os.listdir(\".\\DataSet\\EMCI\")\n",
    "listofCN = os.listdir(\".\\DataSet\\CN\")\n",
    "print(\"start\")\n",
    "ext = Extractor()\n",
    "print(\"End\")\n",
    "\n",
    "for i in range(len(listofCN)):\n",
    "    print(\"collecting 3 brains\")\n",
    "    #CN\n",
    "    print(len(Brains))\n",
    "    print(len(Brain_Types))\n",
    "    file = os.path.join(\".\\DataSet\\CN\",listofCN[i])\n",
    "    img = nib.load(file).get_fdata()\n",
    "    Brain_Data_reduced = Reduce_Brain(img)\n",
    "    Brains.append(Brain_Data_reduced)\n",
    "    Brain_Types.append(CN)\n",
    "    #EMCI\n",
    "    file = os.path.join(\".\\DataSet\\EMCI\",listofEMCI[i])\n",
    "    img = nib.load(file).get_fdata()\n",
    "    Brain_Data_reduced = Reduce_Brain(img)\n",
    "    Brains.append(Brain_Data_reduced)\n",
    "    Brain_Types.append(EMCI)\n",
    "    #LMCI\n",
    "    file = os.path.join(\".\\DataSet\\LMCI\",listofLMCI[i])\n",
    "    img = nib.load(file).get_fdata()\n",
    "    Brain_Data_reduced = Reduce_Brain(img)\n",
    "    Brains.append(Brain_Data_reduced)\n",
    "    Brain_Types.append(LMCI)\n",
    "print(len(Brains))\n",
    "print(len(Brain_Types))\n",
    "print(len(Brains[0]))\n",
    "print(Brain_Types[0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(Brains, Brain_Types, test_size=0.33)\n",
    "model = MLPClassifier(solver='sgd', alpha = 0.0001, hidden_layer_sizes=(70,70,70), activation = 'tanh', learning_rate = 'invscaling')\n",
    "\n",
    "trained_model = model.fit(X_train, y_train)\n",
    "predict = trained_model.predict(X_test)\n",
    "print(predict)\n",
    "print(y_test)\n",
    "print(len(predict))\n",
    "print(len(y_test))\n",
    "f= open(\"Data.txt\",\"w+\")\n",
    "for i in range(len(predict)):\n",
    "    f.write(\"Prediction: \",predict[i])\n",
    "    f.write(\"Acctual: \",y_test[i])\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from deepbrain import Extractor\n",
    "Data_small = []\n",
    "# Load a nifti as 3d numpy image [H, W, D]\n",
    "img = nib.load(\"ADNI_002_S_0295_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070319113623975_S13408_I45108.nii\").get_fdata()\n",
    "\n",
    "ext = Extractor()\n",
    "\n",
    "# `prob` will be a 3d numpy image containing probability \n",
    "# of being brain tissue for each of the voxels in `img`\n",
    "prob = ext.run(img) \n",
    "# mask can be obtained as:\n",
    "mask = prob > 0.5\n",
    "for i in range (len(mask)):\n",
    "    for j in range(len(mask[i])):\n",
    "        for k in range(len(mask[i][j])):\n",
    "            if mask[i][j][k]:\n",
    "                Data_small.append(img[i][j][k])\n",
    "                \n",
    "print(len(Data_small))\n",
    "print(len(img))\n",
    "print(len(img[0]))\n",
    "print(len(img[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start brain extraction model\n",
      "End brain extraction model\n",
      "Brain Reduction Start\n",
      "Brain size pre processed 1218158\n",
      "Diffrence needed 218158\n",
      "Brain size post processed 1000000\n",
      "Brain Reduction End\n",
      "./CN/CN_Number 0.txt\n",
      "Brain Reduction Start\n",
      "Brain size pre processed 1029956\n",
      "Diffrence needed 29956\n",
      "Brain size post processed 1000000\n",
      "Brain Reduction End\n",
      "./EMCI/EMCI_Number 0.txt\n",
      "Brain Reduction Start\n",
      "Brain size pre processed 1004163\n",
      "Diffrence needed 4164\n",
      "Brain size post processed 1000000\n",
      "Brain Reduction End\n",
      "./LMCI/LMCI_Number 0.txt\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepbrain import Extractor\n",
    "import scikitplot as skplt\n",
    "Brain_Size = 1000000\n",
    "CN = 0\n",
    "EMCI = 1\n",
    "LMCI = 2\n",
    "\n",
    "Brain_Data = []\n",
    "Brain_Data_reduced = []\n",
    "Brains = []\n",
    "Brain_Types = []\n",
    "listofLMCI = []\n",
    "listofEMCI = []\n",
    "listofCN = []\n",
    " \n",
    "def Reduce_Brain(img):\n",
    "    print(\"Brain Reduction Start\")\n",
    "    Brain_Data_Small = []\n",
    "    prob = ext.run(img) \n",
    "    mask = prob > 0.5\n",
    "    for i in range (len(mask)):\n",
    "        for j in range(len(mask[i])):\n",
    "            for k in range(len(mask[i][j])):\n",
    "                if mask[i][j][k]:\n",
    "                    Brain_Data_Small.append(img[i][j][k])\n",
    "    print(\"Brain size pre processed\", len(Brain_Data_Small))\n",
    "    if(len(Brain_Data_Small) != Brain_Size):\n",
    "        difrence = len(Brain_Data_Small) - Brain_Size\n",
    "        if(difrence < 0):\n",
    "            amount = (difrence*-1)/2\n",
    "            amount = int(round(amount))\n",
    "            print(\"Diffrence needed\", amount*2)\n",
    "            for i in range(amount):\n",
    "                Brain_Data_Small.insert(0, 0)\n",
    "                Brain_Data_Small.append(0)\n",
    "        if(difrence > 0):\n",
    "            amount = difrence/2\n",
    "            amount = int(round(amount))\n",
    "            print(\"Diffrence needed\", amount*2)\n",
    "            for i in range(amount):\n",
    "                Brain_Data_Small.pop(0)\n",
    "                Brain_Data_Small.pop(len(Brain_Data_Small)-1)\n",
    "    if(len(Brain_Data_Small) > Brain_Size):\n",
    "        Brain_Data_Small.pop(0)\n",
    "    if(len(Brain_Data_Small) < Brain_Size):\n",
    "        Brain_Data_Small.append(0)\n",
    "    print(\"Brain size post processed\",len(Brain_Data_Small))\n",
    "    print(\"Brain Reduction End\")\n",
    "    return Brain_Data_Small\n",
    "\n",
    "listofLMCI = os.listdir(\".\\DataSet\\LMCI\")\n",
    "listofEMCI = os.listdir(\".\\DataSet\\EMCI\")\n",
    "listofCN = os.listdir(\".\\DataSet\\CN\")\n",
    "print(\"Start brain extraction model\")\n",
    "ext = Extractor()\n",
    "print(\"End brain extraction model\")\n",
    "\n",
    "for i in range(len(listofCN)):\n",
    "    #CN\n",
    "    file = os.path.join(\".\\DataSet\\CN\",listofCN[i])\n",
    "    img = nib.load(file).get_fdata()\n",
    "    Brain_Data_reduced = Reduce_Brain(img)\n",
    "    filename = \"./CN/CN_Number \"+ str(i) +\".txt\"\n",
    "    print(filename)\n",
    "    f= open(filename,\"w+\")\n",
    "    for j in range(len(Brain_Data_reduced)):\n",
    "        f.write(\"%d\\n\" % Brain_Data_reduced[j])\n",
    "    f.close()\n",
    "    #EMCI\n",
    "    file = os.path.join(\".\\DataSet\\EMCI\",listofEMCI[i])\n",
    "    img = nib.load(file).get_fdata()\n",
    "    Brain_Data_reduced = Reduce_Brain(img)\n",
    "    filename = \"./EMCI/EMCI_Number \"+ str(i) +\".txt\"\n",
    "    print(filename)\n",
    "    f= open(filename,\"w+\")\n",
    "    for j in range(len(Brain_Data_reduced)):\n",
    "        f.write(\"%d\\n\" % Brain_Data_reduced[j])\n",
    "    f.close()\n",
    "    #LMCI\n",
    "    file = os.path.join(\".\\DataSet\\LMCI\",listofLMCI[i])\n",
    "    img = nib.load(file).get_fdata()\n",
    "    Brain_Data_reduced = Reduce_Brain(img)\n",
    "    filename = \"./LMCI/LMCI_Number \"+ str(i) +\".txt\"\n",
    "    print(filename)\n",
    "    f= open(filename,\"w+\")\n",
    "    for j in range(len(Brain_Data_reduced)):\n",
    "        f.write(\"%d\\n\" % Brain_Data_reduced[j])\n",
    "    f.close()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "722\n",
      "0\n",
      "2447\n",
      "0\n",
      "2447\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nibabel as nb\n",
    "from deepbrain import Extractor\n",
    "import scikitplot as skplt\n",
    "import time\n",
    "Brain_Size = 1400000\n",
    "CN = 0\n",
    "EMCI = 1\n",
    "LMCI = 2\n",
    "\n",
    "Brain_Data = []\n",
    "Brains = []\n",
    "Brain_Types = []\n",
    "listofLMCI = []\n",
    "listofEMCI = []\n",
    "listofCN = []\n",
    "\n",
    "listofLMCI = os.listdir(\"./LMCI\")\n",
    "listofEMCI = os.listdir(\"./EMCI\")\n",
    "listofCN = os.listdir(\"./CN\")\n",
    "\n",
    "_min = 10000\n",
    "_max = 0\n",
    "\n",
    "for i in range(len(listofCN)):\n",
    "    print(len(Brains))\n",
    "    print(len(Brain_Types))\n",
    "    file = os.path.join(\"./CN\",listofCN[i])\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            temp = int(line.strip())\n",
    "            if temp > _max:\n",
    "                _max = temp\n",
    "            elif temp < _min:\n",
    "                _min = temp             \n",
    "    print(_max)\n",
    "    print(_min)\n",
    "    #Brains.append(contents)\n",
    "    #EMCI\n",
    "    file = os.path.join(\"./EMCI\",listofEMCI[i])\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            temp = int(line.strip())\n",
    "            if temp > _max:\n",
    "                _max = temp\n",
    "            elif temp < _min:\n",
    "                _min = temp             \n",
    "    print(_max)\n",
    "    print(_min)\n",
    "    #LMCI\n",
    "    file = os.path.join(\"./LMCI\",listofLMCI[i])\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            temp = int(line.strip())\n",
    "            if temp > _max:\n",
    "                _max = temp\n",
    "            elif temp < _min:\n",
    "                _min = temp             \n",
    "    print(_max)\n",
    "    print(_min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#A = min data B = max data a = min after b = max after x = data to normalize\n",
    "#a + (x-A)*(b-a)/(B-A)\n",
    "import os\n",
    "A = 0\n",
    "B = 5643\n",
    "a = 0\n",
    "b = 1\n",
    "\n",
    "\n",
    "Brain_Data = []\n",
    "listofLMCI = []\n",
    "listofEMCI = []\n",
    "listofCN = []\n",
    "\n",
    "listofLMCI = os.listdir(\"./LMCI\")\n",
    "listofEMCI = os.listdir(\"./EMCI\")\n",
    "listofCN = os.listdir(\"./CN\")\n",
    "\n",
    "for i in range(len(listofCN)):\n",
    "    print(i)\n",
    "    Brain_Data = []\n",
    "    #cn\n",
    "    file = os.path.join(\"./CN\",listofCN[i])\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            x = int(line.strip())\n",
    "            toadd = a + (x-A)*(b-a)/(B-A)\n",
    "            Brain_Data.append(toadd)\n",
    "    with open(file, 'w') as f:\n",
    "        for item in Brain_Data:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    #emci\n",
    "    Brain_Data = []\n",
    "    file = os.path.join(\"./EMCI\",listofEMCI[i])\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            x = int(line.strip())\n",
    "            toadd = a + (x-A)*(b-a)/(B-A)\n",
    "            Brain_Data.append(toadd)\n",
    "    with open(file, 'w') as f:\n",
    "        for item in Brain_Data:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    #lmci\n",
    "    Brain_Data = []\n",
    "    file = os.path.join(\"./LMCI\",listofLMCI[i])\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            x = int(line.strip())\n",
    "            toadd = a + (x-A)*(b-a)/(B-A)\n",
    "            Brain_Data.append(toadd)\n",
    "    with open(file, 'w') as f:\n",
    "        for item in Brain_Data:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
