{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "grand-smell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1000, 1000,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "b1_cnv2d_1 (Conv2D)             (None, 500, 500, 16) 144         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "b1_relu_1 (ReLU)                (None, 500, 500, 16) 0           b1_cnv2d_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "b1_bn_1 (BatchNormalization)    (None, 500, 500, 16) 64          b1_relu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "b1_cnv2d_2 (Conv2D)             (None, 250, 250, 32) 512         b1_bn_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "b1_relu_2 (ReLU)                (None, 250, 250, 32) 0           b1_cnv2d_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "b1_out (BatchNormalization)     (None, 250, 250, 32) 128         b1_relu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "b2_cnv2d_1 (Conv2D)             (None, 250, 250, 32) 1024        b1_out[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "b2_relu_1 (ReLU)                (None, 250, 250, 32) 0           b2_cnv2d_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "b2_bn_1 (BatchNormalization)    (None, 250, 250, 32) 128         b2_relu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 250, 250, 32) 0           b1_out[0][0]                     \n",
      "                                                                 b2_bn_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "b2_cnv2d_2 (Conv2D)             (None, 125, 125, 64) 18432       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "b2_relu_2 (ReLU)                (None, 125, 125, 64) 0           b2_cnv2d_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "b2_bn_2 (BatchNormalization)    (None, 125, 125, 64) 256         b2_relu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "b3_cnv2d_1 (Conv2D)             (None, 125, 125, 64) 4096        b2_bn_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "b3_relu_1 (ReLU)                (None, 125, 125, 64) 0           b3_cnv2d_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "b3_bn_1 (BatchNormalization)    (None, 125, 125, 64) 256         b3_relu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 125, 125, 64) 0           b2_bn_2[0][0]                    \n",
      "                                                                 b3_bn_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "b3_cnv2d_2 (Conv2D)             (None, 63, 63, 128)  73728       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "b3_relu_2 (ReLU)                (None, 63, 63, 128)  0           b3_cnv2d_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "b3_out (BatchNormalization)     (None, 63, 63, 128)  512         b3_relu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 128)          0           b3_out[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "model_output (Dense)            (None, 10)           1290        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 100,570\n",
      "Trainable params: 99,898\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x23686338d48>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.applications import mobilenet_v2, mobilenet, resnet50, densenet\n",
    "from keras.layers import Dense, MaxPooling2D, Conv2D, Flatten, \\\n",
    "    BatchNormalization, Activation, GlobalAveragePooling2D, DepthwiseConv2D, \\\n",
    "    Dropout, ReLU, Concatenate, Input, add\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.callbacks import CSVLogger\n",
    "from datetime import datetime\n",
    "\n",
    "import os.path\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from scipy.spatial import distance\n",
    "import scipy.io as sio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sample_res_net_v0(self,input_shape, output_shape):\n",
    "\n",
    "    input = Input(shape=(input_shape[0], input_shape[1], input_shape[2]))\n",
    "\n",
    "    #block_1\n",
    "    b1_cnv2d_1 = Conv2D(filters=16, kernel_size=(3, 3), strides=(2, 2), padding='same',\n",
    "                     use_bias=False, name='b1_cnv2d_1', kernel_initializer='normal')(input)\n",
    "    b1_relu_1 = ReLU(name='b1_relu_1')(b1_cnv2d_1)\n",
    "    b1_bn_1 = BatchNormalization(epsilon=1e-3, momentum=0.999, name='b1_bn_1')(b1_relu_1)  # size: 14*14\n",
    "\n",
    "    b1_cnv2d_2 = Conv2D(filters=32, kernel_size=(1, 1), strides=(2, 2), padding='same',\n",
    "                        use_bias=False, name='b1_cnv2d_2', kernel_initializer='normal')(b1_bn_1)\n",
    "    b1_relu_2 = ReLU(name='b1_relu_2')(b1_cnv2d_2)\n",
    "    b1_out = BatchNormalization(epsilon=1e-3, momentum=0.999, name='b1_out')(b1_relu_2)  # size: 14*14\n",
    "\n",
    "    #block 2\n",
    "    b2_cnv2d_1 = Conv2D(filters=32, kernel_size=(1, 1), strides=(1, 1), padding='same',\n",
    "                        use_bias=False, name='b2_cnv2d_1', kernel_initializer='normal')(b1_out)\n",
    "    b2_relu_1 = ReLU(name='b2_relu_1')(b2_cnv2d_1)\n",
    "    b2_bn_1 = BatchNormalization(epsilon=1e-3, momentum=0.999, name='b2_bn_1')(b2_relu_1)  # size: 14*14\n",
    "\n",
    "    b2_add = add([b1_out, b2_bn_1])  #\n",
    "\n",
    "    b2_cnv2d_2 = Conv2D(filters=64, kernel_size=(3, 3), strides=(2, 2), padding='same',\n",
    "                        use_bias=False, name='b2_cnv2d_2', kernel_initializer='normal')(b2_add)\n",
    "    b2_relu_2 = ReLU(name='b2_relu_2')(b2_cnv2d_2)\n",
    "    b2_out = BatchNormalization(epsilon=1e-3, momentum=0.999, name='b2_bn_2')(b2_relu_2)  # size: 7*7\n",
    "\n",
    "    #block 3\n",
    "    b3_cnv2d_1 = Conv2D(filters=64, kernel_size=(1, 1), strides=(1, 1), padding='same',\n",
    "                        use_bias=False, name='b3_cnv2d_1', kernel_initializer='normal')(b2_out)\n",
    "    b3_relu_1 = ReLU(name='b3_relu_1')(b3_cnv2d_1)\n",
    "    b3_bn_1 = BatchNormalization(epsilon=1e-3, momentum=0.999, name='b3_bn_1')(b3_relu_1)  # size: 7*7\n",
    "\n",
    "    b3_add = add([b2_out, b3_bn_1])  #\n",
    "\n",
    "    b3_cnv2d_2 = Conv2D(filters=128, kernel_size=(3, 3), strides=(2, 2), padding='same',\n",
    "                        use_bias=False, name='b3_cnv2d_2', kernel_initializer='normal')(b3_add)\n",
    "    b3_relu_2 = ReLU(name='b3_relu_2')(b3_cnv2d_2)\n",
    "    b3_out = BatchNormalization(epsilon=1e-3, momentum=0.999, name='b3_out')(b3_relu_2)  # size: 3*3\n",
    "\n",
    "    #block 4\n",
    "    b4_avg_p = GlobalAveragePooling2D()(b3_out)\n",
    "    output = Dense(output_shape, name='model_output', activation='softmax',\n",
    "                   kernel_initializer='he_uniform')(b4_avg_p)\n",
    "\n",
    "    model = Model(input, output)\n",
    "\n",
    "    model_json = model.to_json()\n",
    "\n",
    "    with open(\"sample_res_net_v0.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "black-module",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9c9188884732>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-9c9188884732>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(input_shape, output_shape)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;34m'''create categorical labels'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sam\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     76\u001b[0m   \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m   \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m   \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m   \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.callbacks import CSVLogger\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import save, load, asarray\n",
    "import os.path\n",
    "from keras import losses\n",
    "from keras import backend as K\n",
    "Brains = []\n",
    "Brain_Types = []\n",
    "Brain_Data = []\n",
    "listofLMCI = []\n",
    "listofEMCI = []\n",
    "listofCN = []\n",
    "\n",
    "listofLMCI = os.listdir(\"./LMCI\")\n",
    "listofEMCI = os.listdir(\"./EMCI\")\n",
    "listofCN = os.listdir(\"./CN\")\n",
    "CN = 0\n",
    "EMCI = 1\n",
    "LMCI = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(input_shape,output_shape):\n",
    "    for i in range(len(listofCN)):\n",
    "        print(i)\n",
    "        Brain_Data = []\n",
    "        #cn\n",
    "        file = os.path.join(\"./CN\",listofCN[i])\n",
    "        with open(file) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                x = float(line.strip())\n",
    "                Brain_Data.append(x)\n",
    "        Brains.append(Brain_Data)\n",
    "        Brain_Types.append(CN)\n",
    "\n",
    "        #emci\n",
    "        Brain_Data = []\n",
    "        file = os.path.join(\"./EMCI\",listofEMCI[i])\n",
    "        with open(file) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                x = float(line.strip())\n",
    "                Brain_Data.append(x)\n",
    "        Brains.append(Brain_Data)\n",
    "        Brain_Types.append(EMCI)\n",
    "        #lmci\n",
    "        Brain_Data = []\n",
    "        file = os.path.join(\"./LMCI\",listofLMCI[i])\n",
    "        with open(file) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                x = float(line.strip())\n",
    "                Brain_Data.append(x)\n",
    "        Brains.append(Brain_Data)\n",
    "        Brain_Types.append(LMCI)\n",
    "    \n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(Brains, Brain_Types, test_size=0.16)\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    '''take 5,000 samples from train to be used as validation set'''\n",
    "    x_val = x_train[-1:]\n",
    "    y_val = y_train[-1:]\n",
    "    x_train = x_train[:-1]\n",
    "    y_train = y_train[:-1]\n",
    "\n",
    "    '''adopting the images shape, from ?*28*28 to ?*28*28*1'''\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "    x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], x_val.shape[2], 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "\n",
    "    '''normalizing images'''\n",
    "    x_train = x_train/255.0\n",
    "    x_val = x_val/255.0\n",
    "    x_test = x_test/255.0\n",
    "\n",
    "    '''create categorical labels'''\n",
    "    y_train = keras.utils.to_categorical(y_train, output_shape)\n",
    "    y_val = keras.utils.to_categorical(y_val, output_shape)\n",
    "    y_test = keras.utils.to_categorical(y_test, output_shape)\n",
    "\n",
    "    '''creating model:'''\n",
    "    model_1 = sample_res_net_v0(input_shape=input_shape, output_shape=output_shape)\n",
    "\n",
    "    '''preparing network for being trained'''\n",
    "    model_1.compile(loss=losses.categorical_crossentropy,\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "    '''start training the model'''\n",
    "    history = model_1.fit(x_train, y_train,\n",
    "                          batch_size=100,\n",
    "                          epochs=2,\n",
    "                          validation_data=(x_val, y_val))\n",
    "    model_1.save(\"res_model.hd5\")\n",
    "\n",
    "    self.show_figures(history.history)\n",
    "\n",
    "\n",
    "def show_figures(self, history):\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.savefig('accuracy')\n",
    "\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.savefig('loss')\n",
    "\n",
    "print(\"Start\")\n",
    "train_model([1000, 1000, 1],3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-quality",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
